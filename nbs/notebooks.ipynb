{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73ce0d3-e5b8-4364-8843-3daefc17acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fcbdd-3c6b-45cd-9760-75190aa381d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f921c53-6feb-42a5-a8f9-fbaa604cd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.scholarshipregion.com/dreamrite-college-scholarship/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76a70c7c-d6fb-4860-b3d9-89f7f4bab1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with webdriver.Chrome(options=options) as driver:\n",
    "#     driver.get(url)\n",
    "#     html = driver.page_source\n",
    "#     time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0133ff-7d16-4971-9997-2b1dd2f0eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection\n",
    "from selenium.webdriver.common.by import By\n",
    "AUTH = 'brd-customer-hl_72e118a5-zone-scraping_browser2:m2z0q1qt88n9'\n",
    "SBR_WEBDRIVER = f'https://{AUTH}@brd.superproxy.io:9515'\n",
    "def scrape(url=None):\n",
    "    print('Connecting to Browser API...')\n",
    "    sbr_connection = ChromiumRemoteConnection(SBR_WEBDRIVER, 'goog', 'chrome')\n",
    "    with Remote(sbr_connection, options=ChromeOptions()) as driver:\n",
    "        print('Connected! Navigating...')\n",
    "        driver.get(url)\n",
    "        print('Taking page screenshot to file page.png')\n",
    "        driver.get_screenshot_as_file('./page.png')\n",
    "        print('Navigated! Scraping page content...')\n",
    "        html = driver.page_source\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4fdd7-ba61-4884-b7dd-7e4415db0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8653b14d-9dbe-454f-81b8-1cfe99311fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\obiez\\\\OneDrive\\\\Desktop\\\\project1\\\\scholarscope'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95824102-176f-46a5-9d61-0a1eaf14280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ade097d-846b-45de-8f82-661b0a38c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup.init_django('scholarscope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9c525f-d3af-499e-9e99-fb328386fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "# html = helpers.scrape(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18e97da-d2d6-41b2-8466-e9d224021b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing for: https://www.scholarshipregion.com/category/undergraduate-scholarships/\n",
      "============================================================\n",
      "STEP 1: Extracting scholarship URLs from list page...\n",
      "Starting local Chrome browser...\n",
      "Local Chrome browser started!\n",
      "Navigating to scholarship list: https://www.scholarshipregion.com/category/undergraduate-scholarships/\n",
      "Taking screenshot of list page...\n",
      "Extracting scholarships from page 1...\n",
      "Found 42 scholarships on page 1\n",
      "No more pages available.\n",
      "Total scholarships found: 42\n",
      "Browser connection closed.\n",
      "Found 42 scholarship links to process\n",
      "\n",
      "STEP 2: Scraping detailed information for each scholarship...\n",
      "\n",
      "Processing 1/42: TotalEnergies National Merit Scholarship 2025 For ...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://www.scholarshipregion.com/totalenergies-national-merit-scholarship/...\n",
      "Data extraction completed!\n",
      "Browser connection closed.\n",
      "âœ“ Successfully scraped details\n",
      "Waiting 3 seconds before next request...\n",
      "\n",
      "Processing 2/42: David Oyedepo Foundation Scholarship (DOF) 2025 Fo...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://www.scholarshipregion.com/david-oyedepo-foundation-scholarship/...\n",
      "Data extraction completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002CF8B42DBE0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\obiez\\OneDrive\\Desktop\\project1\\scholarscope\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "  File \"C:\\Users\\obiez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py\", line 1477, in enumerate\n",
      "    def enumerate():\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(helpers.main_scholarship_scraper(\"https://www.scholarshipregion.com/category/undergraduate-scholarships/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561211ad-45ad-4387-b7e5-9c6bdd7d021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de6b4932-ec94-4188-a467-4fe36cf03dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c950f-e4f3-465a-be09-eff650b3bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956604eb-5a01-4a4a-bc5c-2ecbc19e09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "from decouple import config\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ScholarshipListScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.base_url = None\n",
    "        \n",
    "    def connect_browser(self):\n",
    "        \"\"\"Connect to local Chrome browser\"\"\"\n",
    "        print('Starting local Chrome browser...')\n",
    "        \n",
    "        # Chrome options for anti-detection\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # Optional: Add headless mode\n",
    "        options.add_argument('--headless')\n",
    "        \n",
    "        # Optional: Specify ChromeDriver path if not in PATH\n",
    "        # service = Service('/path/to/chromedriver')\n",
    "        # self.driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        # Use ChromeDriver from PATH\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        # Anti-detection script\n",
    "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        print('Local Chrome browser started!')\n",
    "    \n",
    "    def scrape_scholarship_list(self, list_url, max_scholarships=None):\n",
    "        \"\"\"\n",
    "        Scrape a page containing multiple scholarship links\n",
    "        Returns list of scholarship URLs with basic info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.connect_browser()\n",
    "            self.base_url = f\"{urlparse(list_url).scheme}://{urlparse(list_url).netloc}\"\n",
    "            \n",
    "            print(f'Navigating to scholarship list: {list_url}')\n",
    "            self.driver.get(list_url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Take screenshot for debugging\n",
    "            print('Taking screenshot of list page...')\n",
    "            self.driver.get_screenshot_as_file('./scholarship_list_page.png')\n",
    "            \n",
    "            # Handle pagination if needed\n",
    "            all_scholarship_links = []\n",
    "            page_num = 1\n",
    "            \n",
    "            while True:\n",
    "                print(f'Extracting scholarships from page {page_num}...')\n",
    "                \n",
    "                # Extract scholarship links from current page\n",
    "                page_links = self.extract_scholarship_links_from_page()\n",
    "                \n",
    "                if not page_links:\n",
    "                    print(\"No more scholarship links found.\")\n",
    "                    break\n",
    "                \n",
    "                all_scholarship_links.extend(page_links)\n",
    "                print(f'Found {len(page_links)} scholarships on page {page_num}')\n",
    "                \n",
    "                # Check if we've reached the maximum\n",
    "                if max_scholarships and len(all_scholarship_links) >= max_scholarships:\n",
    "                    all_scholarship_links = all_scholarship_links[:max_scholarships]\n",
    "                    print(f'Reached maximum of {max_scholarships} scholarships')\n",
    "                    break\n",
    "                \n",
    "                # Try to go to next page\n",
    "                if not self.go_to_next_page():\n",
    "                    print(\"No more pages available.\")\n",
    "                    break\n",
    "                \n",
    "                page_num += 1\n",
    "                time.sleep(2)  # Be respectful with requests\n",
    "            \n",
    "            print(f'Total scholarships found: {len(all_scholarship_links)}')\n",
    "            return all_scholarship_links\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping scholarship list: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print('Browser connection closed.')\n",
    "    \n",
    "    def extract_scholarship_links_from_page(self):\n",
    "        \"\"\"Extract scholarship links from the current page\"\"\"\n",
    "        scholarship_links = []\n",
    "        \n",
    "        # Common selectors for scholarship links\n",
    "        link_selectors = [\n",
    "            # Generic article/post links\n",
    "            \"article a[href]\",\n",
    "            \".post a[href]\",\n",
    "            \".entry a[href]\",\n",
    "            \n",
    "            # Scholarship-specific selectors\n",
    "            \"a[href*='scholarship']\",\n",
    "            \"a[href*='grant']\",\n",
    "            \"a[href*='award']\",\n",
    "            \"a[href*='fellowship']\",\n",
    "            \n",
    "            # Title-based selectors\n",
    "            \"h1 a, h2 a, h3 a, h4 a\",\n",
    "            \".title a\",\n",
    "            \".post-title a\",\n",
    "            \".entry-title a\",\n",
    "            \n",
    "            # List-based selectors\n",
    "            \"ul li a[href]\",\n",
    "            \"ol li a[href]\",\n",
    "            \n",
    "            # Card/box-based layouts\n",
    "            \".card a[href]\",\n",
    "            \".box a[href]\",\n",
    "            \".item a[href]\",\n",
    "            \n",
    "            # WordPress/blog specific\n",
    "            \".wp-block-post-title a\",\n",
    "            \".post-link a\",\n",
    "            \n",
    "            # Table-based layouts\n",
    "            \"table td a[href]\",\n",
    "            \"tbody tr a[href]\"\n",
    "        ]\n",
    "        \n",
    "        processed_urls = set()\n",
    "        \n",
    "        for selector in link_selectors:\n",
    "            try:\n",
    "                links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                for link in links:\n",
    "                    try:\n",
    "                        href = link.get_attribute('href')\n",
    "                        text = link.text.strip()\n",
    "                        \n",
    "                        if not href or not text:\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert relative URLs to absolute\n",
    "                        full_url = urljoin(self.base_url, href)\n",
    "                        \n",
    "                        # Skip if already processed\n",
    "                        if full_url in processed_urls:\n",
    "                            continue\n",
    "                        \n",
    "                        # Filter out non-scholarship links\n",
    "                        if self.is_scholarship_link(href, text):\n",
    "                            scholarship_info = {\n",
    "                                'title': text,\n",
    "                                'url': full_url,\n",
    "                                'extracted_from': selector\n",
    "                            }\n",
    "                            \n",
    "                            # Try to get additional info from parent elements\n",
    "                            additional_info = self.extract_additional_info(link)\n",
    "                            scholarship_info.update(additional_info)\n",
    "                            \n",
    "                            scholarship_links.append(scholarship_info)\n",
    "                            processed_urls.add(full_url)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates and sort by relevance\n",
    "        unique_scholarships = []\n",
    "        seen_titles = set()\n",
    "        \n",
    "        for scholarship in scholarship_links:\n",
    "            title_clean = re.sub(r'[^\\w\\s]', '', scholarship['title'].lower())\n",
    "            if title_clean not in seen_titles and len(title_clean) > 10:\n",
    "                unique_scholarships.append(scholarship)\n",
    "                seen_titles.add(title_clean)\n",
    "        \n",
    "        return unique_scholarships\n",
    "    \n",
    "    def is_scholarship_link(self, href, text):\n",
    "        \"\"\"Determine if a link is likely a scholarship link\"\"\"\n",
    "        # URL-based filtering\n",
    "        url_keywords = ['scholarship', 'grant', 'award', 'fellowship', 'bursary', 'funding']\n",
    "        url_lower = href.lower()\n",
    "        \n",
    "        # Text-based filtering\n",
    "        text_keywords = ['scholarship', 'grant', 'award', 'fellowship', 'bursary', 'funding', 'opportunity']\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Exclude unwanted links\n",
    "        exclude_keywords = [\n",
    "            'contact', 'about', 'home', 'login', 'register', 'privacy', \n",
    "            'terms', 'cookie', 'sitemap', 'rss', 'feed', 'category',\n",
    "            'tag', 'author', 'archive', 'search', 'facebook', 'twitter',\n",
    "            'instagram', 'linkedin', 'youtube', 'whatsapp', 'telegram'\n",
    "        ]\n",
    "        \n",
    "        # Check if URL or text contains scholarship keywords\n",
    "        has_scholarship_keyword = (\n",
    "            any(keyword in url_lower for keyword in url_keywords) or\n",
    "            any(keyword in text_lower for keyword in text_keywords)\n",
    "        )\n",
    "        \n",
    "        # Check if it's not an excluded link\n",
    "        not_excluded = not any(keyword in url_lower or keyword in text_lower for keyword in exclude_keywords)\n",
    "        \n",
    "        # Additional filters\n",
    "        is_valid_length = len(text) > 10 and len(text) < 200\n",
    "        is_not_just_numbers = not text.isdigit()\n",
    "        \n",
    "        return has_scholarship_keyword and not_excluded and is_valid_length and is_not_just_numbers\n",
    "    \n",
    "    def extract_additional_info(self, link_element):\n",
    "        \"\"\"Extract additional information around the link\"\"\"\n",
    "        additional_info = {}\n",
    "        \n",
    "        try:\n",
    "            # Try to find deadline information\n",
    "            parent = link_element.find_element(By.XPATH, \"./..\")\n",
    "            parent_text = parent.text\n",
    "            \n",
    "            # Look for dates\n",
    "            date_patterns = [\n",
    "                r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}\\b',\n",
    "                r'\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b',\n",
    "                r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2},?\\s+\\d{4}\\b',\n",
    "                r'\\b\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{4}\\b',\n",
    "                r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b'\n",
    "            ]\n",
    "            \n",
    "            for pattern in date_patterns:\n",
    "                dates = re.findall(pattern, parent_text, re.IGNORECASE)\n",
    "                if dates:\n",
    "                    additional_info['potential_deadline'] = dates[0]\n",
    "                    break\n",
    "            \n",
    "            # Look for monetary amounts\n",
    "            money_pattern = r'[\\$â‚¦Â£â‚¬Â¥]\\s*[\\d,]+(?:\\.\\d{2})?'\n",
    "            amounts = re.findall(money_pattern, parent_text)\n",
    "            if amounts:\n",
    "                additional_info['potential_amount'] = amounts[0]\n",
    "            \n",
    "            # Look for country/location info\n",
    "            countries = ['Nigeria', 'USA', 'UK', 'Canada', 'Australia', 'Germany', 'France']\n",
    "            for country in countries:\n",
    "                if country.lower() in parent_text.lower():\n",
    "                    additional_info['potential_location'] = country\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return additional_info\n",
    "    \n",
    "    def go_to_next_page(self):\n",
    "        \"\"\"Try to navigate to the next page\"\"\"\n",
    "        next_selectors = [\n",
    "            \"a[href*='page'][href*='2']\",\n",
    "            \".next-page a\",\n",
    "            \".pagination .next a\",\n",
    "            \"a:contains('Next')\",\n",
    "            \"a:contains('>')\",\n",
    "            \".wp-pagenavi .next a\",\n",
    "            \".page-numbers.next\"\n",
    "        ]\n",
    "        \n",
    "        for selector in next_selectors:\n",
    "            try:\n",
    "                if ':contains(' in selector:\n",
    "                    # Use XPath for text-based selection\n",
    "                    text = selector.split(':contains(')[1].split(')')[0].strip(\"'\\\"\")\n",
    "                    xpath = f\"//a[contains(text(), '{text}')]\"\n",
    "                    next_links = self.driver.find_elements(By.XPATH, xpath)\n",
    "                else:\n",
    "                    next_links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                \n",
    "                if next_links:\n",
    "                    next_link = next_links[0]\n",
    "                    if next_link.is_enabled():\n",
    "                        self.driver.execute_script(\"arguments[0].click();\", next_link)\n",
    "                        time.sleep(3)  # Wait for page to load\n",
    "                        return True\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17621f92-d97a-4e94-800f-549ef3a97b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest to be used\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from datetime import datetime\n",
    "import re\n",
    "# Remove the decouple import since we're not using environment variables\n",
    "# from decouple import config\n",
    "\n",
    "class ScholarshipSeleniumScraper:\n",
    "    def __init__(self, chrome_driver_path=None):\n",
    "        self.driver = None\n",
    "        self.chrome_driver_path = chrome_driver_path  # Optional: specify chromedriver path\n",
    "        \n",
    "    def connect_browser(self):\n",
    "        \"\"\"Connect to local Chrome browser\"\"\"\n",
    "        print('Starting local Chrome browser...')\n",
    "        \n",
    "        # Set up Chrome options\n",
    "        options = ChromeOptions()\n",
    "        \n",
    "        # Add options to avoid detection and improve performance\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # Optional: Run in headless mode (uncomment if you don't want to see the browser)\n",
    "        # options.add_argument('--headless')\n",
    "        \n",
    "        # Optional: Set window size\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        try:\n",
    "            # Method 1: If you have chromedriver in your PATH or specified path\n",
    "            if self.chrome_driver_path:\n",
    "                service = Service(self.chrome_driver_path)\n",
    "                self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            else:\n",
    "                # Method 2: Let Selenium manage chromedriver automatically (Selenium 4.6+)\n",
    "                self.driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "            # Execute script to hide webdriver property\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            print('Chrome browser started successfully!')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error starting Chrome browser: {str(e)}\")\n",
    "            print(\"\\nTroubleshooting tips:\")\n",
    "            print(\"1. Make sure Chrome browser is installed\")\n",
    "            print(\"2. Install/update chromedriver: pip install --upgrade chromedriver-autoinstaller\")\n",
    "            print(\"3. Or manually download chromedriver from https://chromedriver.chromium.org/\")\n",
    "            raise\n",
    "    \n",
    "        \n",
    "    def scrape_scholarship_data(self, url):\n",
    "        \"\"\"Scrape scholarship data optimized for Django model\"\"\"\n",
    "        try:\n",
    "            self.connect_browser()\n",
    "            \n",
    "            print(f'Navigating to {url}...')\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Extract data matching Django model fields\n",
    "            scholarship_data = {\n",
    "                'title': self.extract_title(),\n",
    "                'description': self.extract_description(),\n",
    "                'reward': self.extract_reward(),\n",
    "                'link': self.extract_application_link(),\n",
    "                'start_date': self.extract_start_date(),\n",
    "                'end_date': self.extract_end_date(),\n",
    "                'requirements': self.extract_requirements(),\n",
    "                'eligibility': self.extract_eligibility(),\n",
    "                'tags': self.extract_tags(),\n",
    "                # 'active': self.determine_if_active(),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            print('Data extraction completed!')\n",
    "            return scholarship_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print('Browser connection closed.')\n",
    "     \n",
    "    def extract_application_link(self):\n",
    "        \"\"\"Extract application link\"\"\"\n",
    "        try:\n",
    "            # Look for application buttons/links\n",
    "            app_selectors = [\n",
    "                \"a[href*='apply']\",\n",
    "                \"a[href*='application']\",\n",
    "                \"button[onclick*='apply']\",\n",
    "                \"a:contains('Apply')\",\n",
    "                \"a:contains('Submit')\",\n",
    "                \".apply-btn\",\n",
    "                \".application-link\"\n",
    "            ]\n",
    "            \n",
    "            for selector in app_selectors:\n",
    "                try:\n",
    "                    if ':contains(' in selector:\n",
    "                        # Use XPath for text-based selection\n",
    "                        xpath = f\"//a[contains(text(), '{selector.split(':contains(')[1].split(')')[0].strip(\"'\")}')]\"\n",
    "                        elements = self.driver.find_elements(By.XPATH, xpath)\n",
    "                    else:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    \n",
    "                    if elements:\n",
    "                        element = elements[0]\n",
    "                        return {\n",
    "                            'text': element.text.strip(),\n",
    "                            'url': element.get_attribute('href') or element.get_attribute('onclick')\n",
    "                        }\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Look for forms with application-related action\n",
    "            forms = self.driver.find_elements(By.TAG_NAME, \"form\")\n",
    "            for form in forms:\n",
    "                action = form.get_attribute('action')\n",
    "                if action and any(keyword in action.lower() for keyword in ['apply', 'application', 'submit']):\n",
    "                    return {\n",
    "                        'text': 'Application form',\n",
    "                        'url': action\n",
    "                    }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting application link: {e}\")\n",
    "        \n",
    "        return \"Application link not found\"\n",
    "    \n",
    "    \n",
    "    def extract_title(self):\n",
    "        \"\"\"Extract scholarship title\"\"\"\n",
    "        selectors = [\n",
    "            \"h1\",\n",
    "            \".entry-title\", \n",
    "            \".post-title\",\n",
    "            \".scholarship-title\",\n",
    "            \"[class*='title']\"\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                element = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                title = element.text.strip()\n",
    "                if title and len(title) > 5:\n",
    "                    # Clean up title\n",
    "                    title = title.split('|')[0].strip()\n",
    "                    title = title.split(' - ')[0].strip()\n",
    "                    return title[:255]  # Match Django CharField max_length\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        \n",
    "        # Fallback to page title\n",
    "        try:\n",
    "            return self.driver.title.split('|')[0].strip()[:255]\n",
    "        except:\n",
    "            return \"Scholarship Title Not Found\"\n",
    "\n",
    "    def extract_description(self):\n",
    "        \"\"\"Extract scholarship description\"\"\"\n",
    "        description_parts = []\n",
    "        \n",
    "        # Strategy 1: Meta description\n",
    "        try:\n",
    "            meta_desc = self.driver.find_element(By.CSS_SELECTOR, 'meta[name=\"description\"]')\n",
    "            content = meta_desc.get_attribute('content')\n",
    "            if content and len(content) > 50:\n",
    "                description_parts.append(content.strip())\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Strategy 2: First few paragraphs\n",
    "        try:\n",
    "            paragraphs = self.driver.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "            content_paragraphs = []\n",
    "            for p in paragraphs[:5]:\n",
    "                text = p.text.strip()\n",
    "                if len(text) > 50:\n",
    "                    # Skip navigation/footer text\n",
    "                    skip_words = ['home', 'menu', 'navigation', 'copyright', 'privacy', 'cookie']\n",
    "                    if not any(skip_word in text.lower() for skip_word in skip_words):\n",
    "                        content_paragraphs.append(text)\n",
    "                        if len(content_paragraphs) >= 2:\n",
    "                            break\n",
    "            \n",
    "            description_parts.extend(content_paragraphs)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Strategy 3: Article content\n",
    "        try:\n",
    "            article_selectors = ['.entry-content', '.post-content', '.article-content', 'article']\n",
    "            for selector in article_selectors:\n",
    "                try:\n",
    "                    element = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    text = element.text.strip()\n",
    "                    if len(text) > 100:\n",
    "                        description_parts.append(text[:500])\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if description_parts:\n",
    "            description = ' '.join(description_parts)\n",
    "            description = re.sub(r'\\s+', ' ', description)  # Clean whitespace\n",
    "            return description\n",
    "        \n",
    "        return \"No description available\"\n",
    "\n",
    "    def extract_reward(self):\n",
    "        \"\"\"Extract scholarship reward/amount\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            \n",
    "            # Nigerian Naira patterns\n",
    "            naira_patterns = [\n",
    "                r'â‚¦\\s*([0-9,]+(?:\\.[0-9]{2})?)',\n",
    "                r'N\\s*([0-9,]+(?:\\.[0-9]{2})?)',\n",
    "                r'([0-9,]+(?:\\.[0-9]{2})?)\\s*naira'\n",
    "            ]\n",
    "            \n",
    "            # USD patterns  \n",
    "            usd_patterns = [\n",
    "                r'\\$\\s*([0-9,]+(?:\\.[0-9]{2})?)',\n",
    "                r'([0-9,]+(?:\\.[0-9]{2})?)\\s*(?:USD|dollars?)'\n",
    "            ]\n",
    "            \n",
    "            # General patterns\n",
    "            general_patterns = [\n",
    "                r'worth\\s*(?:of\\s*)?â‚¦?\\$?\\s*([0-9,]+)',\n",
    "                r'value\\s*(?:of\\s*)?â‚¦?\\$?\\s*([0-9,]+)',\n",
    "                r'amount\\s*(?:of\\s*)?â‚¦?\\$?\\s*([0-9,]+)'\n",
    "            ]\n",
    "            \n",
    "            all_patterns = naira_patterns + usd_patterns + general_patterns\n",
    "            \n",
    "            for pattern in all_patterns:\n",
    "                matches = re.findall(pattern, page_text, re.IGNORECASE)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        if isinstance(match, tuple):\n",
    "                            match = match[0]\n",
    "                        try:\n",
    "                            num_value = float(match.replace(',', ''))\n",
    "                            if num_value > 1000:  # Reasonable minimum\n",
    "                                return f\"â‚¦{match}\" if any(p in pattern for p in naira_patterns) else f\"${match}\"\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            # Look for non-monetary rewards\n",
    "            reward_keywords = ['tuition', 'allowance', 'stipend', 'support', 'funding', 'full scholarship']\n",
    "            for keyword in reward_keywords:\n",
    "                if keyword in page_text.lower():\n",
    "                    return f\"Educational {keyword}\"\n",
    "            \n",
    "            return \"Amount not specified\"\n",
    "            \n",
    "        except:\n",
    "            return \"Amount not specified\"\n",
    "\n",
    "    def extract_start_date(self):\n",
    "        \"\"\"Extract application start date/opening date\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            \n",
    "            # Look for start date patterns\n",
    "            start_patterns = [\n",
    "                r'application\\s*(?:opens?|starts?)[:\\s]*([^.!?\\n]+)',\n",
    "                r'opening\\s*date[:\\s]*([^.!?\\n]+)',\n",
    "                r'start\\s*date[:\\s]*([^.!?\\n]+)',\n",
    "                r'begins?[:\\s]*([^.!?\\n]+)',\n",
    "                r'from[:\\s]*([^.!?\\n]+?)(?:\\s*to\\s*|\\s*-\\s*)',\n",
    "                r'available\\s*from[:\\s]*([^.!?\\n]+)',\n",
    "                r'registration\\s*(?:opens?|starts?)[:\\s]*([^.!?\\n]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in start_patterns:\n",
    "                matches = re.findall(pattern, page_text, re.IGNORECASE)\n",
    "                if matches:\n",
    "                    date_text = matches[0].strip()\n",
    "                    parsed_date = self.parse_date_string(date_text)\n",
    "                    if parsed_date:\n",
    "                        return parsed_date.isoformat()\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extract_end_date(self):\n",
    "        \"\"\"Extract application deadline/end date\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            \n",
    "            # Look for deadline patterns\n",
    "            deadline_patterns = [\n",
    "                r'deadline[:\\s]*([^.!?\\n]+)',\n",
    "                r'due date[:\\s]*([^.!?\\n]+)',\n",
    "                r'closing date[:\\s]*([^.!?\\n]+)',\n",
    "                r'last date[:\\s]*([^.!?\\n]+)',\n",
    "                r'application closes[:\\s]*([^.!?\\n]+)',\n",
    "                r'expires?[:\\s]*([^.!?\\n]+)',\n",
    "                r'until[:\\s]*([^.!?\\n]+)',\n",
    "                r'by[:\\s]*([^.!?\\n]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in deadline_patterns:\n",
    "                matches = re.findall(pattern, page_text, re.IGNORECASE)\n",
    "                if matches:\n",
    "                    date_text = matches[0].strip()\n",
    "                    parsed_date = self.parse_date_string(date_text)\n",
    "                    if parsed_date:\n",
    "                        return parsed_date.isoformat()\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extract_requirements(self):\n",
    "        \"\"\"Extract scholarship requirements/documents needed\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            requirements = []\n",
    "            \n",
    "            # Look for requirement sections\n",
    "            requirement_patterns = [\n",
    "                r'requirements?[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'documents?\\s*(?:required|needed)[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'application\\s*requirements?[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'needed\\s*documents?[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'submit[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'must\\s*(?:provide|submit|include)[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)'\n",
    "            ]\n",
    "            \n",
    "            # Try to find structured requirement lists\n",
    "            try:\n",
    "                # Look for list elements that might contain requirements\n",
    "                list_selectors = [\n",
    "                    'ul li', 'ol li', '.requirements li', '.documents li',\n",
    "                    '[class*=\"requirement\"] li', '[class*=\"document\"] li'\n",
    "                ]\n",
    "                \n",
    "                for selector in list_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        for element in elements:\n",
    "                            text = element.text.strip()\n",
    "                            if self.is_requirement_text(text):\n",
    "                                requirements.append(text)\n",
    "                                if len(requirements) >= 10:  # Limit to 10 requirements\n",
    "                                    break\n",
    "                        if requirements:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # If no structured lists found, use pattern matching\n",
    "            if not requirements:\n",
    "                for pattern in requirement_patterns:\n",
    "                    matches = re.findall(pattern, page_text, re.IGNORECASE | re.MULTILINE)\n",
    "                    if matches:\n",
    "                        requirement_text = matches[0].strip()\n",
    "                        # Split into individual requirements\n",
    "                        req_lines = self.parse_requirement_text(requirement_text)\n",
    "                        requirements.extend(req_lines)\n",
    "                        if requirements:\n",
    "                            break\n",
    "            \n",
    "            # Look for common requirement keywords if nothing found\n",
    "            if not requirements:\n",
    "                common_requirements = self.extract_common_requirements(page_text)\n",
    "                requirements.extend(common_requirements)\n",
    "            \n",
    "            # Clean and format requirements\n",
    "            cleaned_requirements = []\n",
    "            for req in requirements[:10]:  # Limit to 10 requirements\n",
    "                req = req.strip()\n",
    "                if len(req) > 10 and len(req) < 200:  # Reasonable length\n",
    "                    # Remove bullet points and numbering\n",
    "                    req = re.sub(r'^[\\d\\.\\)\\-\\*\\â€¢\\â–º\\â†’\\âž¤]\\s*', '', req)\n",
    "                    req = re.sub(r'^\\w\\)\\s*', '', req)  # Remove a), b), etc.\n",
    "                    cleaned_requirements.append(req.strip())\n",
    "            \n",
    "            return cleaned_requirements if cleaned_requirements else [\"Requirements not specified\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting requirements: {str(e)}\")\n",
    "            return [\"Requirements not specified\"]\n",
    "    \n",
    "\n",
    "    def extract_eligibility(self):\n",
    "        \"\"\"Extract scholarship eligibility criteria\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            eligibility = []\n",
    "            \n",
    "            # Look for eligibility sections\n",
    "            eligibility_patterns = [\n",
    "                r'eligibility[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'eligible\\s*(?:candidates?|applicants?)[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'who\\s*can\\s*apply[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'criteria[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'qualifications?[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)',\n",
    "                r'must\\s*be[:\\s]*([^.!?\\n]*(?:\\n[^.!?\\n]*)*)'\n",
    "            ]\n",
    "            \n",
    "            # Try to find structured eligibility lists\n",
    "            try:\n",
    "                # Look for list elements that might contain eligibility\n",
    "                list_selectors = [\n",
    "                    'ul li', 'ol li', '.eligibility li', '.criteria li',\n",
    "                    '[class*=\"eligibility\"] li', '[class*=\"criteria\"] li',\n",
    "                    '[class*=\"qualification\"] li'\n",
    "                ]\n",
    "                \n",
    "                for selector in list_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        for element in elements:\n",
    "                            text = element.text.strip()\n",
    "                            if self.is_eligibility_text(text):\n",
    "                                eligibility.append(text)\n",
    "                                if len(eligibility) >= 10:  # Limit to 10 criteria\n",
    "                                    break\n",
    "                        if eligibility:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # If no structured lists found, use pattern matching\n",
    "            if not eligibility:\n",
    "                for pattern in eligibility_patterns:\n",
    "                    matches = re.findall(pattern, page_text, re.IGNORECASE | re.MULTILINE)\n",
    "                    if matches:\n",
    "                        eligibility_text = matches[0].strip()\n",
    "                        # Split into individual criteria\n",
    "                        eligibility_lines = self.parse_eligibility_text(eligibility_text)\n",
    "                        eligibility.extend(eligibility_lines)\n",
    "                        if eligibility:\n",
    "                            break\n",
    "            \n",
    "            # Look for common eligibility keywords if nothing found\n",
    "            if not eligibility:\n",
    "                common_eligibility = self.extract_common_eligibility(page_text)\n",
    "                eligibility.extend(common_eligibility)\n",
    "            \n",
    "            # Clean and format eligibility\n",
    "            cleaned_eligibility = []\n",
    "            for criteria in eligibility[:10]:  # Limit to 10 criteria\n",
    "                criteria = criteria.strip()\n",
    "                if len(criteria) > 10 and len(criteria) < 200:  # Reasonable length\n",
    "                    # Remove bullet points and numbering\n",
    "                    criteria = re.sub(r'^[\\d\\.\\)\\-\\*\\â€¢\\â–º\\â†’\\âž¤]\\s*', '', criteria)\n",
    "                    criteria = re.sub(r'^\\w\\)\\s*', '', criteria)  # Remove a), b), etc.\n",
    "                    cleaned_eligibility.append(criteria.strip())\n",
    "            \n",
    "            return cleaned_eligibility if cleaned_eligibility else [\"Eligibility criteria not specified\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting eligibility: {str(e)}\")\n",
    "            return [\"Eligibility criteria not specified\"]\n",
    "\n",
    "    def is_requirement_text(self, text):\n",
    "        \"\"\"Check if text looks like a requirement\"\"\"\n",
    "        requirement_keywords = [\n",
    "            'transcript', 'certificate', 'cv', 'resume', 'letter', 'essay',\n",
    "            'statement', 'recommendation', 'reference', 'passport', 'photo',\n",
    "            'application form', 'birth certificate', 'identification',\n",
    "            'academic record', 'degree', 'diploma', 'waec', 'jamb', 'ssce',\n",
    "            'bank statement', 'financial', 'medical report', 'upload', 'submit'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return (any(keyword in text_lower for keyword in requirement_keywords) and\n",
    "                len(text) > 15 and len(text) < 200)\n",
    "\n",
    "    def is_eligibility_text(self, text):\n",
    "        \"\"\"Check if text looks like eligibility criteria\"\"\"\n",
    "        eligibility_keywords = [\n",
    "            'citizen', 'age', 'year', 'grade', 'gpa', 'cgpa', 'score',\n",
    "            'level', 'undergraduate', 'graduate', 'student', 'enrolled',\n",
    "            'admitted', 'nationality', 'resident', 'income', 'family',\n",
    "            'female', 'male', 'minority', 'disability', 'field of study',\n",
    "            'department', 'faculty', 'university', 'college'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return (any(keyword in text_lower for keyword in eligibility_keywords) and\n",
    "                len(text) > 15 and len(text) < 200)\n",
    "\n",
    "    def parse_requirement_text(self, text):\n",
    "        \"\"\"Parse requirement text into individual requirements\"\"\"\n",
    "        requirements = []\n",
    "        \n",
    "        # Try splitting by common delimiters\n",
    "        delimiters = ['\\n', ';', 'â€¢', 'â–º', 'â†’', 'âž¤']\n",
    "        \n",
    "        for delimiter in delimiters:\n",
    "            if delimiter in text:\n",
    "                parts = text.split(delimiter)\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if self.is_requirement_text(part):\n",
    "                        requirements.append(part)\n",
    "                if requirements:\n",
    "                    return requirements\n",
    "        \n",
    "        # If no clear delimiters, look for sentence patterns\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if self.is_requirement_text(sentence):\n",
    "                requirements.append(sentence)\n",
    "        \n",
    "        return requirements if requirements else [text.strip()]\n",
    "\n",
    "    def parse_eligibility_text(self, text):\n",
    "        \"\"\"Parse eligibility text into individual criteria\"\"\"\n",
    "        eligibility = []\n",
    "        \n",
    "        # Try splitting by common delimiters\n",
    "        delimiters = ['\\n', ';', 'â€¢', 'â–º', 'â†’', 'âž¤']\n",
    "        \n",
    "        for delimiter in delimiters:\n",
    "            if delimiter in text:\n",
    "                parts = text.split(delimiter)\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if self.is_eligibility_text(part):\n",
    "                        eligibility.append(part)\n",
    "                if eligibility:\n",
    "                    return eligibility\n",
    "        \n",
    "        # If no clear delimiters, look for sentence patterns\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if self.is_eligibility_text(sentence):\n",
    "                eligibility.append(sentence)\n",
    "        \n",
    "        return eligibility if eligibility else [text.strip()]\n",
    "\n",
    "    def extract_common_requirements(self, page_text):\n",
    "        \"\"\"Extract common requirement patterns from text\"\"\"\n",
    "        requirements = []\n",
    "        text_lower = page_text.lower()\n",
    "        \n",
    "        # Common document requirements\n",
    "        document_patterns = {\n",
    "            'Academic Transcript': ['transcript', 'academic record'],\n",
    "            'CV/Resume': ['cv', 'resume', 'curriculum vitae'],\n",
    "            'Passport Photograph': ['passport photo', 'recent photo'],\n",
    "            'Birth Certificate': ['birth certificate'],\n",
    "            'Letter of Recommendation': ['recommendation letter', 'reference letter'],\n",
    "            'Statement of Purpose': ['statement of purpose', 'personal statement'],\n",
    "            'Application Form': ['application form', 'completed form'],\n",
    "            'Academic Certificates': ['certificate', 'degree certificate'],\n",
    "            'Identification Document': ['id card', 'identification', 'national id']\n",
    "        }\n",
    "        \n",
    "        for req_name, keywords in document_patterns.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                requirements.append(req_name)\n",
    "        \n",
    "        return requirements\n",
    "\n",
    "    def extract_common_eligibility(self, page_text):\n",
    "        \"\"\"Extract common eligibility patterns from text\"\"\"\n",
    "        eligibility = []\n",
    "        text_lower = page_text.lower()\n",
    "        \n",
    "        # Age requirements\n",
    "        age_match = re.search(r'(?:age|years?)\\s*(?:between|from)?\\s*(\\d+)(?:\\s*(?:to|and|-)\\s*(\\d+))?', text_lower)\n",
    "        if age_match:\n",
    "            if age_match.group(2):\n",
    "                eligibility.append(f\"Age between {age_match.group(1)} and {age_match.group(2)} years\")\n",
    "            else:\n",
    "                eligibility.append(f\"Age {age_match.group(1)} years or above\")\n",
    "        \n",
    "        # Educational level\n",
    "        if 'undergraduate' in text_lower:\n",
    "            eligibility.append(\"Must be an undergraduate student\")\n",
    "        if 'graduate' in text_lower or 'postgraduate' in text_lower:\n",
    "            eligibility.append(\"Must be a graduate/postgraduate student\")\n",
    "        \n",
    "        # Nationality/Citizenship\n",
    "        if 'nigerian' in text_lower and 'citizen' in text_lower:\n",
    "            eligibility.append(\"Must be a Nigerian citizen\")\n",
    "        if 'international' in text_lower:\n",
    "            eligibility.append(\"Open to international students\")\n",
    "        \n",
    "        # Academic performance\n",
    "        gpa_match = re.search(r'(?:gpa|cgpa)\\s*(?:of\\s*)?(\\d+\\.?\\d*)', text_lower)\n",
    "        if gpa_match:\n",
    "            eligibility.append(f\"Minimum GPA/CGPA of {gpa_match.group(1)}\")\n",
    "        \n",
    "        # Gender requirements\n",
    "        if 'female' in text_lower and 'only' in text_lower:\n",
    "            eligibility.append(\"Female students only\")\n",
    "        if 'male' in text_lower and 'only' in text_lower:\n",
    "            eligibility.append(\"Male students only\")\n",
    "        \n",
    "        return eligibility\n",
    "        \"\"\"Extract tags/categories for the scholarship\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text.lower()\n",
    "            title_text = self.driver.title.lower()\n",
    "            \n",
    "            # Combine all text for analysis\n",
    "            all_text = f\"{page_text} {title_text}\"\n",
    "            \n",
    "            tags = set()\n",
    "            \n",
    "            # Educational level tags\n",
    "            level_keywords = {\n",
    "                'undergraduate': ['undergraduate', 'bachelor', 'bsc', 'ba', 'first degree'],\n",
    "                'postgraduate': ['postgraduate', 'masters', 'msc', 'ma', 'phd', 'doctorate'],\n",
    "                'high school': ['secondary', 'high school', 'ssce', 'waec', 'neco'],\n",
    "                'diploma': ['diploma', 'nd', 'hnd', 'certificate']\n",
    "            }\n",
    "            \n",
    "            # Field of study tags\n",
    "            field_keywords = {\n",
    "                'engineering': ['engineering', 'engineer', 'technology'],\n",
    "                'medicine': ['medicine', 'medical', 'health', 'nursing', 'pharmacy'],\n",
    "                'law': ['law', 'legal', 'jurisprudence'],\n",
    "                'business': ['business', 'management', 'mba', 'finance', 'accounting'],\n",
    "                'science': ['science', 'biology', 'chemistry', 'physics', 'mathematics'],\n",
    "                'arts': ['arts', 'literature', 'history', 'language'],\n",
    "                'agriculture': ['agriculture', 'farming', 'veterinary'],\n",
    "                'computer science': ['computer', 'software', 'programming', 'it', 'technology'],\n",
    "                'education': ['education', 'teaching', 'pedagogy']\n",
    "            }\n",
    "            \n",
    "            # Gender-based tags\n",
    "            gender_keywords = {\n",
    "                'female': ['women', 'female', 'girl', 'ladies'],\n",
    "                'male': ['men', 'male', 'boy', 'gentleman']\n",
    "            }\n",
    "            \n",
    "            # Location-based tags\n",
    "            location_keywords = {\n",
    "                'international': ['international', 'global', 'worldwide', 'abroad'],\n",
    "                'nigeria': ['nigeria', 'nigerian', 'local'],\n",
    "                'africa': ['africa', 'african'],\n",
    "                'usa': ['usa', 'america', 'united states'],\n",
    "                'uk': ['uk', 'britain', 'united kingdom'],\n",
    "                'canada': ['canada', 'canadian']\n",
    "            }\n",
    "            \n",
    "            # Special categories\n",
    "            special_keywords = {\n",
    "                'merit': ['merit', 'academic excellence', 'outstanding'],\n",
    "                'need-based': ['need', 'financial aid', 'low income'],\n",
    "                'minority': ['minority', 'disadvantaged', 'underrepresented'],\n",
    "                'research': ['research', 'thesis', 'dissertation'],\n",
    "                'leadership': ['leadership', 'community service', 'volunteer'],\n",
    "                'sports': ['sports', 'athletic', 'football', 'basketball']\n",
    "            }\n",
    "            \n",
    "            # Check all keyword categories\n",
    "            all_categories = [level_keywords, field_keywords, gender_keywords, \n",
    "                            location_keywords, special_keywords]\n",
    "            \n",
    "            for category in all_categories:\n",
    "                for tag, keywords in category.items():\n",
    "                    if any(keyword in all_text for keyword in keywords):\n",
    "                        tags.add(tag)\n",
    "            \n",
    "            # Look for explicit tags in HTML\n",
    "            try:\n",
    "                # Check meta keywords\n",
    "                meta_keywords = self.driver.find_element(By.CSS_SELECTOR, 'meta[name=\"keywords\"]')\n",
    "                keywords_content = meta_keywords.get_attribute('content')\n",
    "                if keywords_content:\n",
    "                    meta_tags = [tag.strip().lower() for tag in keywords_content.split(',')]\n",
    "                    tags.update(meta_tags[:5])  # Limit to 5 meta tags\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Check for category/tag elements\n",
    "            try:\n",
    "                tag_selectors = [\n",
    "                    '.tags a', '.categories a', '.tag', '.category',\n",
    "                    '[class*=\"tag\"]', '[class*=\"category\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in tag_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        for element in elements[:5]:  # Limit to 5 elements\n",
    "                            tag_text = element.text.strip().lower()\n",
    "                            if tag_text and len(tag_text) < 50:\n",
    "                                tags.add(tag_text)\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Convert to list and limit to reasonable number\n",
    "            final_tags = list(tags)[:10]  # Limit to 10 tags\n",
    "            \n",
    "            # Clean up tags\n",
    "            cleaned_tags = []\n",
    "            for tag in final_tags:\n",
    "                tag = re.sub(r'[^\\w\\s-]', '', tag)  # Remove special characters\n",
    "                tag = re.sub(r'\\s+', ' ', tag).strip()  # Clean whitespace\n",
    "                if len(tag) > 1 and len(tag) < 30:  # Reasonable length\n",
    "                    cleaned_tags.append(tag)\n",
    "            \n",
    "            return cleaned_tags if cleaned_tags else ['general']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting tags: {str(e)}\")\n",
    "            return ['general']\n",
    "\n",
    "    def parse_date_string(self, date_str):\n",
    "        \"\"\"Parse date string to datetime object\"\"\"\n",
    "        date_str = date_str.strip()\n",
    "        \n",
    "        # Remove common prefixes/suffixes\n",
    "        date_str = re.sub(r'^(on|by|from|until|before|after)\\s+', '', date_str, flags=re.IGNORECASE)\n",
    "        date_str = re.sub(r'\\s+(onwards?|forward)$', '', date_str, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Common date formats\n",
    "        date_formats = [\n",
    "            '%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%d',\n",
    "            '%d-%m-%Y', '%B %d, %Y', '%d %B %Y',\n",
    "            '%b %d, %Y', '%d %b %Y', '%Y/%m/%d',\n",
    "            '%d.%m.%Y', '%Y.%m.%d'\n",
    "        ]\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                return datetime.strptime(date_str, fmt)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def extract_tags(self):\n",
    "        \"\"\"Extract tags/categories for the scholarship\"\"\"\n",
    "        try:\n",
    "            page_text = self.driver.find_element(By.TAG_NAME, \"body\").text.lower()\n",
    "            title_text = self.driver.title.lower()\n",
    "            \n",
    "            # Combine all text for analysis\n",
    "            all_text = f\"{page_text} {title_text}\"\n",
    "            \n",
    "            tags = set()\n",
    "            \n",
    "            # Only include sensible/relevant tags\n",
    "            sensible_keywords = {\n",
    "                'undergraduate': ['undergraduate', 'bachelor', 'bsc', 'ba', 'first degree'],\n",
    "                'postgraduate': ['postgraduate', 'masters', 'msc', 'ma', 'phd', 'doctorate'],\n",
    "                'highschool': ['secondary', 'high school', 'ssce', 'waec', 'neco'],\n",
    "                'international': ['international', 'global', 'worldwide', 'abroad'],\n",
    "                'merit': ['merit', 'academic excellence', 'outstanding'],\n",
    "                'need': ['need', 'financial aid', 'low income', 'need-based']\n",
    "            }\n",
    "            \n",
    "            # Check for sensible keywords only\n",
    "            for tag, keywords in sensible_keywords.items():\n",
    "                if any(keyword in all_text for keyword in keywords):\n",
    "                    tags.add(tag)\n",
    "            \n",
    "            # Look for explicit tags in HTML (but filter them)\n",
    "            try:\n",
    "                # Check meta keywords\n",
    "                meta_keywords = self.driver.find_element(By.CSS_SELECTOR, 'meta[name=\"keywords\"]')\n",
    "                keywords_content = meta_keywords.get_attribute('content')\n",
    "                if keywords_content:\n",
    "                    meta_tags = [tag.strip().lower() for tag in keywords_content.split(',')]\n",
    "                    # Only add meta tags that match our sensible keywords\n",
    "                    for meta_tag in meta_tags[:5]:\n",
    "                        if meta_tag in sensible_keywords.keys():\n",
    "                            tags.add(meta_tag)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Check for category/tag elements (but filter them)\n",
    "            try:\n",
    "                tag_selectors = [\n",
    "                    '.tags a', '.categories a', '.tag', '.category',\n",
    "                    '[class*=\"tag\"]', '[class*=\"category\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in tag_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        for element in elements[:5]:  # Limit to 5 elements\n",
    "                            tag_text = element.text.strip().lower()\n",
    "                            # Only add if it matches our sensible keywords\n",
    "                            if tag_text in sensible_keywords.keys():\n",
    "                                tags.add(tag_text)\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Convert to list and ensure we have the sensible tags only\n",
    "            final_tags = [tag for tag in tags if tag in sensible_keywords.keys()]\n",
    "            \n",
    "            return final_tags if final_tags else ['general']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting tags: {str(e)}\")\n",
    "            return ['general']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d93372e-b41b-41c7-bf19-d2761ab14495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ScholarshipBatchProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize with your detailed scholarship scraper function\n",
    "        detail_scraper_func should take a URL and return scholarship details\n",
    "        \"\"\"\n",
    "        \n",
    "        self.scraper = ScholarshipSeleniumScraper()\n",
    "        # self.detail_scraper = scraper.scrape_scholarship_data()\n",
    "        self.list_scraper = ScholarshipListScraper()\n",
    "        \n",
    "    def process_scholarship_list(self, list_url, max_scholarships=None, delay_between_scrapes=3):\n",
    "        \"\"\"\n",
    "        Complete pipeline: Get list of scholarships, then scrape each one for details\n",
    "        Returns nested dictionary structure with batch metadata and scholarship data\n",
    "        \"\"\"\n",
    "        print(f\"Starting batch processing for: {list_url}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Get list of scholarship URLs\n",
    "        print(\"STEP 1: Extracting scholarship URLs from list page...\")\n",
    "        scholarship_links = self.list_scraper.scrape_scholarship_list(list_url, max_scholarships)\n",
    "        \n",
    "        if not scholarship_links:\n",
    "            print(\"No scholarship links found!\")\n",
    "            return self._create_empty_batch_result(list_url)\n",
    "        \n",
    "        print(f\"Found {len(scholarship_links)} scholarship links to process\")\n",
    "        \n",
    "        # Step 2: Process each scholarship for detailed information\n",
    "        print(\"\\nSTEP 2: Scraping detailed information for each scholarship...\")\n",
    "        detailed_scholarships = []\n",
    "        successful_scrapes = 0\n",
    "        failed_scrapes = 0\n",
    "        \n",
    "        for i, scholarship in enumerate(scholarship_links, 1):\n",
    "            print(f\"\\nProcessing {i}/{len(scholarship_links)}: {scholarship['title'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Use your detailed scraper\n",
    "                detailed_data = self.scraper.scrape_scholarship_data((scholarship['url']))\n",
    "                \n",
    "                if detailed_data and isinstance(detailed_data, dict):\n",
    "                    # Use scholarship title/name as the key\n",
    "                    scholarship_name = detailed_data.get('title', scholarship['title'])\n",
    "                    if not scholarship_name:\n",
    "                        scholarship_name = f\"Scholarship_{i}\"\n",
    "                    \n",
    "                    # Create nested structure for each scholarship\n",
    "                    scholarship_entry = {\n",
    "                        'name': scholarship_name,\n",
    "                        'source_info': {\n",
    "                            'list_title': scholarship['title'],\n",
    "                            'source_url': scholarship['url'],\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        },\n",
    "                        'scholarship_data': detailed_data,\n",
    "                        'additional_list_info': {k: v for k, v in scholarship.items() \n",
    "                                               if k not in ['title', 'url']}\n",
    "                    }\n",
    "                    detailed_scholarships.append(scholarship_entry)\n",
    "                    successful_scrapes += 1\n",
    "                    print(f\"âœ“ Successfully scraped details\")\n",
    "                else:\n",
    "                    failed_scrapes += 1\n",
    "                    print(f\"âœ— Failed to scrape details - no data returned\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_scrapes += 1\n",
    "                print(f\"âœ— Error scraping {scholarship['url']}: {str(e)}\")\n",
    "            \n",
    "            # Respectful delay between requests\n",
    "            if i < len(scholarship_links):\n",
    "                print(f\"Waiting {delay_between_scrapes} seconds before next request...\")\n",
    "                time.sleep(delay_between_scrapes)\n",
    "        \n",
    "        # Step 3: Create nested batch result structure\n",
    "        batch_result = self._create_batch_result(\n",
    "            list_url, \n",
    "            detailed_scholarships, \n",
    "            len(scholarship_links),\n",
    "            successful_scrapes,\n",
    "            failed_scrapes\n",
    "        )\n",
    "        \n",
    "        # Step 4: Save comprehensive results (removed - just return the data)\n",
    "        \n",
    "        print(f\"\\nBatch processing completed!\")\n",
    "        print(f\"Successfully scraped {successful_scrapes} out of {len(scholarship_links)} scholarships\")\n",
    "        print(f\"Failed scrapes: {failed_scrapes}\")\n",
    "        \n",
    "        return batch_result\n",
    "    \n",
    "    def _create_batch_result(self, source_url, scholarships, total_found, successful, failed):\n",
    "        \"\"\"Create nested dictionary structure for batch results\"\"\"\n",
    "        return {\n",
    "            'batch_metadata': {\n",
    "                'batch_id': f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                'source_url': source_url,\n",
    "                'processed_at': datetime.now().isoformat(),\n",
    "                'scraper_version': '2.0',\n",
    "                'processing_stats': {\n",
    "                    'total_scholarships_found': total_found,\n",
    "                    'successful_scrapes': successful,\n",
    "                    'failed_scrapes': failed,\n",
    "                    'success_rate': f\"{(successful/total_found*100):.1f}%\" if total_found > 0 else \"0%\"\n",
    "                }\n",
    "            },\n",
    "            'scholarships': {\n",
    "                scholarship['name']: scholarship \n",
    "                for scholarship in scholarships\n",
    "            },\n",
    "            'summary': {\n",
    "                'total_scholarships': len(scholarships),\n",
    "                'scholarship_names': [scholarship['name'] for scholarship in scholarships],\n",
    "                'available_fields': self._get_available_fields(scholarships) if scholarships else []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_empty_batch_result(self, source_url):\n",
    "        \"\"\"Create empty batch result structure when no scholarships found\"\"\"\n",
    "        return {\n",
    "            'batch_metadata': {\n",
    "                'batch_id': f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                'source_url': source_url,\n",
    "                'processed_at': datetime.now().isoformat(),\n",
    "                'scraper_version': '2.0',\n",
    "                'processing_stats': {\n",
    "                    'total_scholarships_found': 0,\n",
    "                    'successful_scrapes': 0,\n",
    "                    'failed_scrapes': 0,\n",
    "                    'success_rate': \"0%\"\n",
    "                }\n",
    "            },\n",
    "            'scholarships': {},\n",
    "            'summary': {\n",
    "                'total_scholarships': 0,\n",
    "                'scholarship_names': [],\n",
    "                'available_fields': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_available_fields(self, scholarships):\n",
    "        \"\"\"Extract available fields from scraped scholarships\"\"\"\n",
    "        if not scholarships:\n",
    "            return []\n",
    "        \n",
    "        all_fields = set()\n",
    "        for scholarship in scholarships:\n",
    "            if 'scholarship_data' in scholarship and isinstance(scholarship['scholarship_data'], dict):\n",
    "                all_fields.update(scholarship['scholarship_data'].keys())\n",
    "        \n",
    "        return sorted(list(all_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b884b9d-6b00-4fe9-acf0-2e2aa717382a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelpers\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c388cf6-0a81-4b1b-b95d-bd42c4cf4b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing for: https://scholarsworld.ng/scholarships/undergraduate-scholarships/\n",
      "============================================================\n",
      "STEP 1: Extracting scholarship URLs from list page...\n",
      "Starting local Chrome browser...\n",
      "Local Chrome browser started!\n",
      "Navigating to scholarship list: https://scholarsworld.ng/scholarships/undergraduate-scholarships/\n",
      "Taking screenshot of list page...\n",
      "Extracting scholarships from page 1...\n",
      "Found 20 scholarships on page 1\n",
      "Reached maximum of 5 scholarships\n",
      "Total scholarships found: 5\n",
      "Browser connection closed.\n",
      "Found 5 scholarship links to process\n",
      "\n",
      "STEP 2: Scraping detailed information for each scholarship...\n",
      "\n",
      "Processing 1/5: SCHOLARSHIPS...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://scholarsworld.ng/scholarships/...\n",
      "Data extraction completed!\n",
      "Browser connection closed.\n",
      "âœ“ Successfully scraped details\n",
      "Waiting 1 seconds before next request...\n",
      "\n",
      "Processing 2/5: Ashesi University Mastercard Scholarship 2025: Ful...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://scholarsworld.ng/ashesi-university-mastercard-scholarship/...\n",
      "Data extraction completed!\n",
      "Browser connection closed.\n",
      "âœ“ Successfully scraped details\n",
      "Waiting 1 seconds before next request...\n",
      "\n",
      "Processing 3/5: NIGERIAN SCHOLARSHIPS...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://scholarsworld.ng/nigerian-scholarships/...\n",
      "Data extraction completed!\n",
      "Browser connection closed.\n",
      "âœ“ Successfully scraped details\n",
      "Waiting 1 seconds before next request...\n",
      "\n",
      "Processing 4/5: EO Foundation Scholarship (Etinosa Oghogho): â‚¦150,...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://scholarsworld.ng/eo-foundation-scholarship-etinosa-oghogho/...\n",
      "Data extraction completed!\n",
      "Browser connection closed.\n",
      "âœ“ Successfully scraped details\n",
      "Waiting 1 seconds before next request...\n",
      "\n",
      "Processing 5/5: Okwun Ojah Education Scholarship Award 2025 For Ni...\n",
      "Starting local Chrome browser...\n",
      "Chrome browser started successfully!\n",
      "Navigating to https://scholarsworld.ng/okwun-ojah-education-scholarship-award/...\n",
      "Data extraction completed!\n",
      "Browser connection closed.\n",
      "âœ“ Successfully scraped details\n",
      "\n",
      "Batch processing completed!\n",
      "Successfully scraped 5 out of 5 scholarships\n",
      "Failed scrapes: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Example 1: Quick test scrape\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# print(\"Example 1: Quick Test Scrape\")\u001b[39;00m\n\u001b[32m    138\u001b[39m     results = quick_scholarship_scrape(\u001b[33m\"\u001b[39m\u001b[33mhttps://scholarsworld.ng/scholarships/undergraduate-scholarships/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m    140\u001b[39m         \u001b[38;5;28mprint\u001b[39m(i, k)\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Example 2: Full production scrape\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# print(\"Example 2: Full Production Scrape\")\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# results = full_scholarship_scrape(\"https://example-scholarships.com/list\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m#         first_scholarship = get_scholarship_by_name(results, all_scholarships[0])\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m#         print(f\"First scholarship data: {first_scholarship}\")\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "list_urls = [\n",
    "        \"https://www.scholarshipregion.com/category/undergraduate-scholarships/\",\n",
    "        \"https://www.scholarshipregion.com/category/postgraduate-scholarships/\",\n",
    "        # Add more list URLs as needed\n",
    "    ]\n",
    "\n",
    "def main_scholarship_scraper(list_url, max_scholarships=None, delay_between_scrapes=3, save_to_file=False):\n",
    "    \"\"\"\n",
    "    Complete scholarship scraping pipeline that combines all three classes:\n",
    "    1. ScholarshipListScraper - extracts scholarship URLs from list pages\n",
    "    2. ScholarshipScraper - scrapes detailed info from individual scholarship pages  \n",
    "    3. ScholarshipBatchProcessor - orchestrates the entire process\n",
    "    \n",
    "    Args:\n",
    "        list_url (str): URL of the page containing scholarship listings\n",
    "        max_scholarships (int, optional): Maximum number of scholarships to process\n",
    "        delay_between_scrapes (int): Seconds to wait between individual scholarship scrapes\n",
    "        save_to_file (bool): Whether to save results to a JSON file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested dictionary with batch metadata and scholarship data\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize the batch processor with the detailed scraper\n",
    "        batch_processor = ScholarshipBatchProcessor()\n",
    "        \n",
    "        # Start the complete pipeline\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Process the scholarship list\n",
    "        results = batch_processor.process_scholarship_list(\n",
    "            list_url=list_url,\n",
    "            max_scholarships=max_scholarships,\n",
    "            delay_between_scrapes=delay_between_scrapes\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def quick_scholarship_scrape(list_url, max_scholarships=5):\n",
    "    \"\"\"\n",
    "    Quick function for testing - scrapes first 5 scholarships with minimal delay\n",
    "    \n",
    "    Args:\n",
    "        list_url (str): URL of scholarship list page\n",
    "        max_scholarships (int): Number of scholarships to scrape (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results dictionary\n",
    "    \"\"\"\n",
    "    return main_scholarship_scraper(\n",
    "        list_url=list_url, \n",
    "        max_scholarships=max_scholarships, \n",
    "        delay_between_scrapes=1,\n",
    "        save_to_file=False\n",
    "    )\n",
    "\n",
    "\n",
    "# def full_scholarship_scrape(list_url, save_results=True):\n",
    "#     \"\"\"\n",
    "#     Full comprehensive scrape with all scholarships found\n",
    "    \n",
    "#     Args:\n",
    "#         list_url (str): URL of scholarship list page\n",
    "#         save_results (bool): Whether to save results to file\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Results dictionary\n",
    "#     \"\"\"\n",
    "#     return main_scholarship_scraper(\n",
    "#         list_url=list_url, \n",
    "#         max_scholarships=None, \n",
    "#         delay_between_scrapes=3,\n",
    "#         save_to_file=save_results\n",
    "#     )\n",
    "\n",
    "\n",
    "# def get_scholarship_by_name(results, scholarship_name):\n",
    "#     \"\"\"\n",
    "#     Helper function to retrieve a specific scholarship by name from results\n",
    "    \n",
    "#     Args:\n",
    "#         results (dict): Results from main_scholarship_scraper\n",
    "#         scholarship_name (str): Name of the scholarship to retrieve\n",
    "    \n",
    "#     Returns:\n",
    "#         dict or None: Scholarship data if found\n",
    "#     \"\"\"\n",
    "#     if not results or 'scholarships' not in results:\n",
    "#         return None\n",
    "    \n",
    "#     scholarships = results['scholarships']\n",
    "    \n",
    "#     # Exact match first\n",
    "#     if scholarship_name in scholarships:\n",
    "#         return scholarships[scholarship_name]\n",
    "    \n",
    "#     # Partial match (case insensitive)\n",
    "#     scholarship_name_lower = scholarship_name.lower()\n",
    "#     for name, data in scholarships.items():\n",
    "#         if scholarship_name_lower in name.lower():\n",
    "#             return data\n",
    "    \n",
    "#     return None\n",
    "\n",
    "\n",
    "# def list_all_scholarships(results):\n",
    "#     \"\"\"\n",
    "#     Helper function to list all scholarship names from results\n",
    "    \n",
    "#     Args:\n",
    "#         results (dict): Results from main_scholarship_scraper\n",
    "    \n",
    "#     Returns:\n",
    "#         list: List of scholarship names\n",
    "#     \"\"\"\n",
    "#     if not results or 'scholarships' not in results:\n",
    "#         return []\n",
    "    \n",
    "#     return list(results['scholarships'].keys())\n",
    "\n",
    "\n",
    "# Example usage functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Quick test scrape\n",
    "    # print(\"Example 1: Quick Test Scrape\")\n",
    "    results = quick_scholarship_scrape(\"https://scholarsworld.ng/scholarships/undergraduate-scholarships/\")\n",
    "    for i, k in results:\n",
    "        print(i, k)\n",
    "    \n",
    "    # Example 2: Full production scrape\n",
    "    # print(\"Example 2: Full Production Scrape\")\n",
    "    # results = full_scholarship_scrape(\"https://example-scholarships.com/list\")\n",
    "    \n",
    "    # Example 3: Custom scrape with specific parameters\n",
    "    # print(\"Example 3: Custom Scrape\")\n",
    "    # results = main_scholarship_scraper(\n",
    "    #     list_url=\"https://example-scholarships.com/list\",\n",
    "    #     max_scholarships=10,\n",
    "    #     delay_between_scrapes=2,\n",
    "    #     save_to_file=True\n",
    "    # )\n",
    "    \n",
    "    # Example 4: Working with results\n",
    "    # if results:\n",
    "    #     # List all scholarships\n",
    "    #     all_scholarships = list_all_scholarships(results)\n",
    "    #     print(f\"Found scholarships: {all_scholarships}\")\n",
    "    #     \n",
    "    #     # Get specific scholarship\n",
    "    #     if all_scholarships:\n",
    "    #         first_scholarship = get_scholarship_by_name(results, all_scholarships[0])\n",
    "    #         print(f\"First scholarship data: {first_scholarship}\")\n",
    "    \n",
    "    print(\"Pipeline functions are ready to use!\")\n",
    "    print(\"Call main_scholarship_scraper() with your target URL to begin scraping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78696904-4340-456d-842a-cbd2c3ea5a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWSING: SCHOLARSHIPS Study abroad programs ::: {'name': 'BROWSING: SCHOLARSHIPS Study abroad programs', 'source_info': {'list_title': 'SCHOLARSHIPS', 'source_url': 'https://scholarsworld.ng/scholarships/', 'scraped_at': '2025-06-10T05:17:31.825871'}, 'scholarship_data': {'title': 'BROWSING: SCHOLARSHIPS Study abroad programs', 'description': 'Explore an unending list of fully funded Ongoing Scholarship Opportunities for international students in 2024 and APPLY. \"At Scholars World, our mission is to empower prospective university students, undergraduates, and postgraduates by providing them with timely educational news and invaluable scholarship opportunities. We are dedicated to equipping individuals with the latest insights and information to make informed decisions about their academic journeys.\" We\\'re social. Connect with us:', 'reward': 'â‚¦150,000', 'link': {'text': '', 'url': 'https://scholarsworld.ng/fme-tvet-application-portal-login/'}, 'start_date': None, 'end_date': None, 'requirements': ['Requirements not specified'], 'eligibility': ['GRADUATE TRAINEES', 'Undergraduate Scholarships', 'Postgraduate Scholarships', 'Upstream Oil and Gas Graduate Trainee Program 2025: Career Opportunity', 'ExxonMobil Graduate Internship Programme 2025: Apply Now', 'King Abdullah University Internship (KAUST Visiting Student Research Program (VSRP)): Fully Funded Opportunity'], 'tags': ['merit', 'undergraduate', 'international', 'postgraduate'], 'scraped_at': '2025-06-10T05:17:29.486255'}, 'additional_list_info': {'extracted_from': 'article a[href]'}}\n",
      "Ashesi University Mastercard Scholarship 2025: Fully Funded to Ghana ::: {'name': 'Ashesi University Mastercard Scholarship 2025: Fully Funded to Ghana', 'source_info': {'list_title': 'Ashesi University Mastercard Scholarship 2025: Fully Funded to Ghana', 'source_url': 'https://scholarsworld.ng/ashesi-university-mastercard-scholarship/', 'scraped_at': '2025-06-10T05:18:19.431251'}, 'scholarship_data': {'title': 'Ashesi University Mastercard Scholarship 2025: Fully Funded to Ghana', 'description': 'TheAshesi University Mastercard Scholarship is fully funded covering all fees and costs for room and board as well as learning resources. Here is another fully funded opportunity for prospective undergraduates: The Ashesi University Mastercard Scholarship. The Mastercard Foundation Scholars Program at Ashesi University is a partnership between the fintech giant based in Canada and one of the best tertiary institutions in Ghana. Here is another fully funded opportunity for prospective undergraduates: The Ashesi University Mastercard Scholarship. The Mastercard Foundation Scholars Program at Ashesi University is a partnership between the fintech giant based in Canada and one of the best tertiary institutions in Ghana. Get Realtime Updates from Scholars World on Google News (Follow us) The entirety of the costs of your tuition, accommodation, living expenses, and comprehensive study materials, including books and a lapto', 'reward': 'â‚¦30,000', 'link': {'text': '', 'url': 'https://scholarsworld.ng/fme-tvet-application-portal-login/'}, 'start_date': None, 'end_date': '2025-12-08T00:00:00', 'requirements': ['Have an excellent academic record', 'Demonstrate financial need', 'Once you apply for admission at Ashesi University, the the Admissions and Financial Aid Office determines which award â€“ including the Mastercard Foundation Scholars Programme award â€“ you may receive.'], 'eligibility': ['GRADUATE TRAINEES', 'Category: Scholarships || Undergraduate Scholarships', 'Participation in an accelerated masterâ€™s programme in partnership with Arizona State University', 'Management Information Systems,', 'Be a citizen of an African country', 'Read the information we have provided on this webpage and ensure your eligibility', 'Click the button below to proceed to the Ashesi University Mastercard Foundation Scholars Program page.', 'Once you apply for admission at Ashesi University, the the Admissions and Financial Aid Office determines which award â€“ including the Mastercard Foundation Scholars Programme award â€“ you may receive.', 'Undergraduate Scholarships', 'Postgraduate Scholarships'], 'tags': ['postgraduate', 'highschool', 'merit', 'undergraduate', 'international', 'need'], 'scraped_at': '2025-06-10T05:18:17.042290'}, 'additional_list_info': {'extracted_from': 'article a[href]'}}\n",
      "BROWSING: NIGERIAN SCHOLARSHIPS Study abroad programs ::: {'name': 'BROWSING: NIGERIAN SCHOLARSHIPS Study abroad programs', 'source_info': {'list_title': 'NIGERIAN SCHOLARSHIPS', 'source_url': 'https://scholarsworld.ng/nigerian-scholarships/', 'scraped_at': '2025-06-10T05:19:09.408306'}, 'scholarship_data': {'title': 'BROWSING: NIGERIAN SCHOLARSHIPS Study abroad programs', 'description': 'Apply for Nigerian Scholarships for undergraduate and postgraduate students. 1. NNPC 2. NLNG 3. Jim Ovia 4. KPMG 5. Federal Government Scholarship 6. PTDF This page contains a list of scholarship opportunities available for Nigerian students. These scholarships offer huge rewards to Nigerian students to study in Nigerian or international universities. \"At Scholars World, our mission is to empower prospective university students, undergraduates, and postgraduates by providing them with timely educational news and invaluable scholarship opportunities. We are dedicated to equipping individuals with the latest insights and information to make informed decisions about their academic journeys.\" We\\'re social. Connect with us:', 'reward': 'â‚¦150,000', 'link': {'text': '', 'url': 'https://scholarsworld.ng/fme-tvet-application-portal-login/'}, 'start_date': None, 'end_date': None, 'requirements': ['Requirements not specified'], 'eligibility': ['GRADUATE TRAINEES', 'Undergraduate Scholarships', 'Postgraduate Scholarships', 'Upstream Oil and Gas Graduate Trainee Program 2025: Career Opportunity', 'ExxonMobil Graduate Internship Programme 2025: Apply Now', 'King Abdullah University Internship (KAUST Visiting Student Research Program (VSRP)): Fully Funded Opportunity'], 'tags': ['merit', 'undergraduate', 'international', 'postgraduate'], 'scraped_at': '2025-06-10T05:19:04.425873'}, 'additional_list_info': {'extracted_from': 'article a[href]', 'potential_location': 'Nigeria'}}\n",
      "EO Foundation Scholarship (Etinosa Oghogho): â‚¦150,000 Opportunity ::: {'name': 'EO Foundation Scholarship (Etinosa Oghogho): â‚¦150,000 Opportunity', 'source_info': {'list_title': 'EO Foundation Scholarship (Etinosa Oghogho): â‚¦150,000 Opportunity', 'source_url': 'https://scholarsworld.ng/eo-foundation-scholarship-etinosa-oghogho/', 'scraped_at': '2025-06-10T05:20:16.131802'}, 'scholarship_data': {'title': 'EO Foundation Scholarship (Etinosa Oghogho): â‚¦150,000 Opportunity', 'description': 'The EO Foundation Scholarship offers a one-time â‚¦150,000 award to selected students of Nigerian universities. Etinosa Oghogho EO Foundation Scholarship: Empowering Nigerian StudentsOnline tutoring The Etinosa Oghogho Foundation is inviting applications from all Nigerian students to apply for her 2025 Academic Scholarship opportunity. Etinosa Oghogho EO Foundation Scholarship: Empowering Nigerian StudentsOnline tutoring The Etinosa Oghogho Foundation is inviting applications from all Nigerian students to apply for her 2025 Academic Scholarship opportunity. X (Twitter) Join the Scholars World community on X (Twitter) to explore the world of opportunities! Successful applicants will be awarded a N150,000 prize to support their educational sojourn. Online tutoring In this post, the full details of the eligibility, benefits, time', 'reward': 'â‚¦150,000', 'link': {'text': '', 'url': 'https://scholarsworld.ng/fme-tvet-application-portal-login/'}, 'start_date': None, 'end_date': '2025-06-30T00:00:00', 'requirements': ['Submit your application as and when due', 'Complete Application Form', 'Academic Transcripts', 'Birth Certificate', 'Fill out the application form and upload the required documents', 'Submit your application and await your decision.'], 'eligibility': ['GRADUATE TRAINEES', 'Category: Scholarships || Undergraduate Scholarships', 'Be a Nigerian university student', 'Details of the University Dean', 'Read all the details provided on this page', 'Click on the button below to visit the application page', 'Undergraduate Scholarships', 'Postgraduate Scholarships', 'Upstream Oil and Gas Graduate Trainee Program 2025: Career Opportunity', 'ExxonMobil Graduate Internship Programme 2025: Apply Now'], 'tags': ['merit', 'undergraduate', 'postgraduate'], 'scraped_at': '2025-06-10T05:20:13.385138'}, 'additional_list_info': {'extracted_from': 'article a[href]', 'potential_amount': 'â‚¦150,000'}}\n",
      "Okwun Ojah Education Scholarship Award 2025 For Nigerians: â‚¦6million Award ::: {'name': 'Okwun Ojah Education Scholarship Award 2025 For Nigerians: â‚¦6million Award', 'source_info': {'list_title': 'Okwun Ojah Education Scholarship Award 2025 For Nigerians: â‚¦6million Award', 'source_url': 'https://scholarsworld.ng/okwun-ojah-education-scholarship-award/', 'scraped_at': '2025-06-10T05:21:08.638645'}, 'scholarship_data': {'title': 'Okwun Ojah Education Scholarship Award 2025 For Nigerians: â‚¦6million Award', 'description': 'The Okwun Ojah Education Scholarship offers a â‚¦6 million award to fresh and returning students enrolled in Law at Nigerian tertiary institutions. The Okwun Ojah Education Scholarship is a â‚¦6m award for eligible students of Nigerian universities. The Okwun Ojah Education Scholarship offers a â‚¦6 million award to fresh and returning students enrolled in Law at Nigerian tertiary institutions. The Okwun Ojah Education Scholarship is a â‚¦6m award for eligible students of Nigerian universities. The Okwun Ojah Education Scholarship offers a â‚¦6 million award to fresh and returning students enrolled in Law at Nigerian tertiary institutions. Online learning Get Realtime Updates from Scholars World on Google News (Follow us) In this post, the full details of the eligibility, benefits, timeline, and application procedure will be provided. Student loans X (Twitter) Join the Scholars World commu', 'reward': 'â‚¦150,000', 'link': {'text': '', 'url': 'https://scholarsworld.ng/fme-tvet-application-portal-login/'}, 'start_date': None, 'end_date': '2025-08-09T00:00:00', 'requirements': ['Must provide two academic references .'], 'eligibility': ['GRADUATE TRAINEES', 'Host: Dr Okwun Ojah Family Foundation', 'Category: Scholarships || Undergraduate Scholarships', 'Must have gained admission into a recognized Nigerian university to study Law.', 'Must be in First (1st) year â€“ Fifth (5th) Year according to their school academic calendar.', 'For 200 level students and above, applicant must have a Cumulative Grade Point Average (CGPA) of 3.0', 'For 100 level students and above, applicant must have a Cumulative Grade Point Average (CGPA) of 3.5', 'Read the information we have provided on this webpage and ensure your eligibility', 'Undergraduate Scholarships', 'Postgraduate Scholarships'], 'tags': ['merit', 'undergraduate', 'postgraduate'], 'scraped_at': '2025-06-10T05:21:04.301860'}, 'additional_list_info': {'extracted_from': 'article a[href]', 'potential_amount': 'â‚¦6', 'potential_location': 'Nigeria'}}\n"
     ]
    }
   ],
   "source": [
    "for k, v in results.get('scholarships').items():\n",
    "    print(f\"{k} ::: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce4bbf-ad9c-481a-8d64-7ecb5891b817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
